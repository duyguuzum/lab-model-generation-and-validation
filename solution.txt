Lab | Model generation, and validation
For this lab, we still keep using the marketing_customer_analysis.csv file that you can find in the files_for_lab folder.

Get the data
We are using the marketing_customer_analysis.csv file.

Linear regression
1-Select the columns which are correlated with total_claim_amount and don't suffer from multicollinearity (see the previous lab)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# The line down below is needed to prevent matplotlib to open the graph in a seaparate popup window. 
%matplotlib inline

data = pd.read_csv('marketing_customer_analysis.csv')

df = pd.DataFrame(data)
corr_matrix = df.corr()

correlation_matrix = data.corrwith(data['Total Claim Amount'])
correlation_matrix 

2-Remove outliers

iqr = np.percentile(data['Total Claim Amount'],75) - np.percentile(data['Total Claim Amount'],25)
upper_wisker = np.percentile(data['Total Claim Amount'],75) + 1.5*iqr
lower_wisker = np.percentile(data['Total Claim Amount'],25) - 1.5*iqr

data = data[(data['Total Claim Amount']>lower_wisker) & (data['Total Claim Amount']<upper_wisker)]
sns.displot(data['Total Claim Amount'])
plt.show()

3-X-y split. (define which column you want to predict, and which ones you will use to make the prediction)

y = data['Total Claim Amount']
X = data.drop(['Total Claim Amount'], axis=1)

4-Use the Train-test split to create the Train, and Test sets (make sure to set the random_state option to any integer number of your choice).

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

5-Use the pd.DataFrame() function to create new Pandas DataFrames from the X_train, and X_test Numpy arrays obtained in the previous step (make sure to use the columns= option to set the columns names to X.columns).

X_train_df = pd.DataFrame(X_train, columns = X.columns)
X_test_df  = pd.DataFrame(X_test,  columns = X.columns)

6-Split the X_train Pandas DataFrame into two: numerical, and categorical using df.select_dtypes().

#Split the X_train Pandas DataFrame into two: numerical, and categorical using df.select_dtypes().
X_train_df_num = X_train_df.select_dtypes(include = np.number)
X_train_df_cat = X_train_df.select_dtypes(include = object)

X_test_df_num = X_test_df.select_dtypes(include = np.number)
X_test_df_cat = X_test_df.select_dtypes(include = object)

7-If you need to transform any column, Train your transformers and/or scalers all the numerical columns using the .fit() only in the Train set (only one transformer/scaler for all the columns, check here, and here using the .transform()

import pickle
from sklearn.preprocessing import MinMaxScaler # Sets for each colum the minimum = 0 and the maximum = 1

scaler = MinMaxScaler()

scaler.fit(X_train_df_num)

X_train_num_scaled = scaler.transform(X_train_df_num) # .transform() applies the transformation x_normalized will be np.array
X_test_num_scaled = scaler.transform(X_test_df_num)

8-Save all your transformers/scalers right after the .fit() using pickle using the code shown below:

import os

path = "transformers/"
# Check whether the specified path exists or not
isExist = os.path.exists(path)
if not isExist:
    # Create a new directory because it does not exist
    os.makedirs(path)
    print("The new directory is created!")

filename = "scalerfile.pkl" # Use a descriptive name for your scaler/transformer but keep the ".pkl" file extension
with open(path+filename, "wb") as file:
    pickle.dump(X_train_num_scaled, file) # Replace "variable" with the name of the variable that contains your transformer

9-If you used a transformer/scaler in the previous step, create new Pandas DataFrames from the Numpy arrays generated by the .transform() using the pd.DataFrame() function as you did earlier with the Numpy arrays generated by the train_test_split() function.

# We create new Pandas DataFrames out of the Numpy arrays.

X_train_num_scaled_df = pd.DataFrame(X_train_num_scaled, columns=X_train_df_num.columns)
X_test_num_scaled_df = pd.DataFrame(X_test_num_scaled, columns=X_test_df_num.columns)

10-Transform the categorical columns into numbers using a:
OneHotEncoder for categorical nominal columns. (again only use the .fit() in the Train set, but the .transform() in the Train and the Test sets)

X_train_df_cat = X_train_df_cat.drop(['Customer','Effective To Date'], axis=1)
X_test_df_cat = X_test_df_cat.drop(['Customer','Effective To Date'], axis=1)

X_train_df_cat.head()

#Transform the categorical columns into numbers using a:
#OneHotEncoder for categorical nominal columns. 
#(again only use the .fit() in the Train set, but the .transform() in the Train and the Test sets)
from sklearn.preprocessing import OneHotEncoder

# Create the OneHotEncoder object
encoder = OneHotEncoder() # categorıes = [ [possıble_values_for_fırst_column], [possıble_values_for_seccond_column],.. ]

# Fit and transform the categorical columns
# encoder.fıt(X_traın_df_cat).transform(X_traın_df_cat)
encoded_data = encoder.fit_transform(X_train_df_cat).toarray()
encoded_data_test = encoder.transform(X_test_df_cat).toarray()

# Create a new DataFrame with the encoded data
encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out())
encoded_df_test = pd.DataFrame(encoded_data_test, columns=encoder.get_feature_names_out())

# Concatenate the encoded DataFrame with the remaining columns from the original data
#final_data = pd.concat([data.drop(columns=categorical_columns), encoded_df], axis=1)

# Print the final data
display(encoded_df)

11-Remember to save all your transformers/scalers right after the .fit() using pickle using the code shown below:

path = "encoders/"
# Check whether the specified path exists or not
isExist = os.path.exists(path)
if not isExist:
    # Create a new directory because it does not exist
    os.makedirs(path)
    print("The new directory is created!")

filename = "transformerfile.pkl" # use a descriptive name for your encoder but keep the ".pkl" file extension
with open(path+filename, "wb") as file:
    pickle.dump(encoded_data, file) # Replace "variable" with the name of the variable that contains your transformer

12-Use .replace() to cast into numbers any categorical ordinal column replacing each label with a number that: respects the order of the labels and the relative "distance"

X_cat = X.select_dtypes(include = object)
X_cat['Response'].replace(to_replace=['Yes', 'No'] ,value=[1, 2], inplace=True)                                                           

13-Concat numerical_transformer and categorical_transfomed DataFrames using pd.concat()

#Concat numerical_transformer and categorical_transfomed DataFrames using pd.concat().
concatenated_df = pd.concat([X_train_num_scaled_df, encoded_df], axis=1)
concatenated_df_test = pd.concat([X_test_num_scaled_df, encoded_df_test], axis=1)

14-Apply another MinMaxScaler to the concatenated DataFrame.

scaler = MinMaxScaler()

concatenated_df_c=scaler.fit_transform(concatenated_df)
concatenated_df_c_test=scaler.transform(concatenated_df_test)

15-Remember to save all your MinMaxScaler right after the .fit() using pickle using the code shown below

path = "scalers/"
# Check whether the specified path exists or not
isExist = os.path.exists(path)
if not isExist:
    # Create a new directory because it does not exist
    os.makedirs(path)
    print("The new directory is created!")

    filename = "scalefilename.pkl" # use a descriptive name for your encoder but keep the ".pkl" file extension
with open(path+filename, "wb") as file:
    pickle.dump(concatenated_df_c, file) # Replace "variable" with the name of the variable that contains your transformer

16-Apply linear regression to the Pandas DataFrame obtained in the previous step using sklearn

from sklearn.linear_model import LinearRegression

# Create a LinearRegression model and fit it to the data
model = LinearRegression()
model.fit(concatenated_df_c, y_train)

# Access the coefficients and intercept
coefficients = model.coef_
intercept = model.intercept_

print("Coefficients:", coefficients)
print("Intercept:", intercept)

# Make predictions using the trained model
predictions_train = model.predict(concatenated_df_c)
print("Predictions:", predictions_train)

prediction_test = model.predict(concatenated_df_c_test)
print("Predictions:", prediction_test)

17-Remember to save your linear model right after the .fit() using pickle using the code shown below

path = "models/"
    # Check whether the specified path exists or not
isExist = os.path.exists(path)
if not isExist:
    # Create a new directory because it does not exist
    os.makedirs(path)
    print("The new directory is created!")

filename = "modelfile.pkl" # use a descriptive name for your encoder but keep the ".pkl" file extension
with open(path+filename, "wb") as file:
    pickle.dump(model, file) # Replace "variable" with the name of the variable that contains your transformer

 18-Model Validation
Compute the following metrics for your Train and Test sets:

R2.
MSE.
RMSE
MAE.

import sklearn.metrics as metrics

mae = metrics.mean_absolute_error(y_train, predictions_train)
mse = metrics.mean_squared_error(y_train, predictions_train)
rmse = np.sqrt(mse) # or mse**(0.5)  
r2 = metrics.r2_score(y_train, predictions_train)

print("Results of sklearn.metrics:")
print("MAE:",mae)
print("MSE:", mse)
print("RMSE:", rmse)
print("R-Squared:", r2)

#^Test error metrıcs

mae = metrics.mean_absolute_error(y_test, prediction_test)
mse = metrics.mean_squared_error(y_test, prediction_test)
rmse = np.sqrt(mse) # or mse**(0.5)  
r2 = metrics.r2_score(y_test, prediction_test)

print("Results of sklearn.metrics:")
print("MAE:",mae)
print("MSE:", mse)
print("RMSE:", rmse)
print("R-Squared:", r2)

19-Create a Pandas DataFrame to summarize the error metrics for the Train and Test sets.

# Define the error metrics and their values
train_metrics = {
    'MSE': 9429.8,
    'RMSE': 97.1,
    'MAE': 74.4
}

test_metrics = {
    'MSE': 9723.9,
    'RMSE': 98.6,
    'MAE': 76.0
}

# Create a DataFrame
df = pd.DataFrame([train_metrics, test_metrics], index=['Train', 'Test'])

# Display the DataFrame
print(df)


                      



